{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9258cf52-b548-42f6-8ce5-2edc96c0dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pickle\n",
    "from openpyxl.reader.excel import load_workbook\n",
    "from openpyxl.workbook import Workbook\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dbe2e8-6b03-4aea-8be9-38174fb61451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now get articles about  SYT\n",
      "search links for symbol  SYT\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-677322c88513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mlink_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[0mlink_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'There are total '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' links in the file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0msave_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-677322c88513>\u001b[0m in \u001b[0;36msearch_links\u001b[1;34m(symbol, num_pages)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_pages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://seekingalpha.com/symbol/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0msymbol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/more_focus?page='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "def search_links(symbol, num_pages = 500):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    link_list = list()\n",
    "    # search all the articles about specific symbol and save it in link_list\n",
    "    print ('search links for symbol ', symbol)\n",
    "    for i in range(1, num_pages):\n",
    "        url = 'https://seekingalpha.com/symbol/'+ symbol + '/more_focus?page=' + str(i)\n",
    "        req = urllib.request.urlopen(url)\n",
    "        html = urllib2.urlopen(req).read()\n",
    "        soup = BeautifulSoup(html)\n",
    "        contents = soup.find_all(\"div\", class_='\\\\\"symbol_article\\\\\"')\n",
    "        if len(contents) == 0:\n",
    "            break\n",
    "        for c in contents:\n",
    "            arti_link = c.findChild('a').get('href', None)\n",
    "            full_link = \"https://seekingalpha.com\" + arti_link.replace('\\\\\"','')\n",
    "            link_list.append(full_link)\n",
    "    print ('Searching links finished.')\n",
    "    return link_list\n",
    "\n",
    "def extract_summary(symbol, link_list):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    content_list = list()\n",
    "    for i in range(len(link_list)):\n",
    "        url = link_list[i]\n",
    "        print ('extract summary from page: ', url)\n",
    "        page_info = dict()\n",
    "        try:\n",
    "            req = urllib2.Request(url, headers=hdr)\n",
    "            html = urllib2.urlopen(req).read()\n",
    "            soup = BeautifulSoup(html)\n",
    "        except:\n",
    "            print ('the %d_th link %s error ' % (i, url))\n",
    "            time.sleep(20)\n",
    "            save_to_excel(symbol, content_list)\n",
    "            content_list = list()\n",
    "            try:\n",
    "                req = urllib2.Request(url, headers=hdr)\n",
    "                html = urllib2.urlopen(req).read()\n",
    "                soup = BeautifulSoup(html)\n",
    "            except:\n",
    "                print ('still cannot open url ', url)\n",
    "                continue\n",
    "\n",
    "        # get the main content\n",
    "        main_content = soup.find_all(\"div\", id = \"content-rail\")\n",
    "        if len(main_content):\n",
    "            main_content = main_content[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # get the published time\n",
    "        publishedTime = main_content.find_all(\"time\", itemprop=\"datePublished\")\n",
    "        if len(publishedTime):\n",
    "            publishedTime = publishedTime[0].get(\"content\", None)\n",
    "        else:\n",
    "            continue\n",
    "        if publishedTime.startswith('2014-02') or publishedTime.startswith('2013'):\n",
    "            break\n",
    "\n",
    "        # get title of the article\n",
    "        title = main_content.find_all(\"h1\", itemprop=\"headline\")\n",
    "        if len(title):\n",
    "            title = title[0].text\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # get author homepage link\n",
    "        author = main_content.find_all(\"a\", class_=\"name-link\")\n",
    "        if len(author):\n",
    "            author = author[0].get(\"href\", None)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # get summary\n",
    "        summary = main_content.find_all(\"div\", class_=\"a-sum\")\n",
    "        if len(summary):\n",
    "            summary = summary[0]\n",
    "            summary_content = list()\n",
    "            paragraphs = summary.find_all(\"p\")\n",
    "            for p in paragraphs:\n",
    "                summary_content.append(p.text)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # save page information\n",
    "        page_info['url'] = url\n",
    "        page_info[\"publishedTime\"] = publishedTime\n",
    "        page_info[\"title\"] = title\n",
    "        page_info[\"author\"] = author\n",
    "        page_info[\"summary\"] = summary_content\n",
    "        content_list.append(page_info)\n",
    "        time.sleep(3)\n",
    "        if len(content_list) == 100:\n",
    "            save_to_excel(symbol, content_list)\n",
    "            content_list = list()\n",
    "    return content_list\n",
    "\n",
    "def save_links(symbol, link_list):\n",
    "    filename = symbol + '_links.pkl'\n",
    "    print ('save links into file ', filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        cPickle.dump(link_list, f)\n",
    "    print ('saving links finished.')\n",
    "\n",
    "def save_content(symbol, content_list):\n",
    "    filename = symbol + '.pkl'\n",
    "    print ('save content into file ', filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        cPickle.dump(content_list, f)\n",
    "    print ('saving content finished.')\n",
    "\n",
    "\n",
    "def save_to_excel(symbol, contents):\n",
    "    filename = symbol + '.xlsx'\n",
    "    print ('save content into ', filename)\n",
    "    if os.path.exists(filename):\n",
    "        wb = load_workbook(filename=filename)\n",
    "        ws = wb[symbol]\n",
    "        row = ws.max_row\n",
    "    else:\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.title = symbol\n",
    "        ws.cell(row=1, column=1, value='Title')\n",
    "        ws.cell(row=1, column=2, value='PageLink')\n",
    "        ws.cell(row=1, column=3, value='Time')\n",
    "        ws.cell(row=1, column=4, value='Author')\n",
    "        ws.cell(row=1, column=5, value='Summary')\n",
    "        row = 1\n",
    "    for i, c in zip(range(row + 1, row + 1 + len(contents)), contents):\n",
    "        ws.cell(row=i, column=1, value=c['title'])\n",
    "        ws.cell(row=i, column=2, value=c['url'])\n",
    "        ws.cell(row=i, column=3, value=c['publishedTime'])\n",
    "        ws.cell(row=i, column=4, value=c['author'])\n",
    "        summary = c['summary']\n",
    "        for j, s in zip(range(len(summary)), summary):\n",
    "            ws.cell(row=i, column=5 + j, value=s)\n",
    "    wb.save(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    symbols = ['SYT', 'VALE']\n",
    "    for symbol in symbols:\n",
    "        print ('now get articles about ', symbol)\n",
    "        linkfile = symbol + '_links.pkl'\n",
    "        if os.path.exists(linkfile):\n",
    "            print( 'load links from existing file ', linkfile)\n",
    "            with open(linkfile, 'r') as f:\n",
    "                link_list = cPickle.load(f)\n",
    "        else:\n",
    "            link_list = search_links(symbol)\n",
    "        print ('There are total ', len(link_list), ' links in the file.')\n",
    "        save_links(symbol, link_list)\n",
    "        content_list = extract_summary(symbol, link_list)\n",
    "        save_to_excel(symbol, content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235ca11-a9ed-4e05-8b80-1b53f0169f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c958c-4690-4493-9aa5-049258667e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
